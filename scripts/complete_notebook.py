"""
Complete the notebook with performance metrics, tuning, and regression
This is the final comprehensive addition to the notebook
"""

import json
from pathlib import Path

def complete_notebook(notebook_path):
    """Add final comprehensive cells"""
    
    with open(notebook_path, 'r') as f:
        notebook = json.load(f)
    
    def create_cell(cell_type, source, execution_count=None):
        cell = {
            "cell_type": cell_type,
            "metadata": {},
            "source": source if isinstance(source, list) else [source]
        }
        if cell_type == "code":
            cell["execution_count"] = execution_count
            cell["outputs"] = []
        return cell
    
    new_cells = []
    
    # =====================================================================
    # PERFORMANCE METRICS
    # =====================================================================
    
    new_cells.append(create_cell("markdown", [
        "### Task 5.b: Performance Metrics\n",
        "\n",
        "Calculate comprehensive performance metrics for all models.\n",
        "\n",
        "**Success Criteria Focus:**\n",
        "- **USE:** Recall (Declined), Precision (Declined), F1-Score, AUC-ROC\n",
        "- **DO NOT USE:** Accuracy alone (due to class imbalance)\n",
        "\n",
        "**Why?** We need to correctly identify \"Declined\" applications to minimize risk."
    ]))
    
    new_cells.append(create_cell("code", [
        "# Task 5.b: Calculate comprehensive performance metrics\n",
        "print(\"=\"*70)\n",
        "print(\"TASK 5.b: PERFORMANCE METRICS (TEST SET)\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "# Helper function to calculate all metrics\n",
        "def calculate_metrics(y_true, y_pred, y_proba, model_name):\n",
        "    \"\"\"Calculate all classification metrics\"\"\"\n",
        "    \n",
        "    # Focus on 'Declined' class as per success criteria\n",
        "    accuracy = accuracy_score(y_true, y_pred)\n",
        "    recall = recall_score(y_true, y_pred, pos_label='Declined')\n",
        "    precision = precision_score(y_true, y_pred, pos_label='Declined', zero_division=0)\n",
        "    f1 = f1_score(y_true, y_pred, pos_label='Declined', zero_division=0)\n",
        "    \n",
        "    # AUC-ROC\n",
        "    y_binary = (y_true == 'Declined').astype(int)\n",
        "    # Get index for 'Declined' class in probability array\n",
        "    try:\n",
        "        declined_idx = list(y_proba.shape[1] - 1 if 'Declined' in str(y_true.unique()) else 1)\n",
        "        if y_proba.shape[1] > 1:\n",
        "            auc = roc_auc_score(y_binary, y_proba[:, 1])  # Assuming Declined is second class\n",
        "        else:\n",
        "            auc = 0.5\n",
        "    except:\n",
        "        auc = 0.5\n",
        "    \n",
        "    return {\n",
        "        'Model': model_name,\n",
        "        'Accuracy': accuracy,\n",
        "        'Recall (Declined)': recall,\n",
        "        'Precision (Declined)': precision,\n",
        "        'F1-Score (Declined)': f1,\n",
        "        'AUC-ROC': auc\n",
        "    }\n",
        "\n",
        "# Calculate metrics for all models\n",
        "y_proba_nb = nb_model.predict_proba(X_test)\n",
        "y_proba_lr = lr_model.predict_proba(X_test)\n",
        "y_proba_rf = rf_model.predict_proba(X_test)\n",
        "\n",
        "metrics_nb = calculate_metrics(y_test, y_pred_nb, y_proba_nb, 'Naive Bayes')\n",
        "metrics_lr = calculate_metrics(y_test, y_pred_lr, y_proba_lr, 'Logistic Regression')\n",
        "metrics_rf = calculate_metrics(y_test, y_pred_rf, y_proba_rf, 'Random Forest')\n",
        "\n",
        "# Create results DataFrame\n",
        "results_df = pd.DataFrame([metrics_nb, metrics_lr, metrics_rf])\n",
        "results_df = results_df.set_index('Model')\n",
        "\n",
        "print(\"\\nPerformance Metrics Summary:\")\n",
        "print(results_df.round(4))\n",
        "\n",
        "# Highlight best scores\n",
        "print(\"\\n\" + \"=\"*70)\n",
        "print(\"BEST SCORES PER METRIC:\")\n",
        "print(\"=\"*70)\n",
        "for metric in results_df.columns:\n",
        "    best_model = results_df[metric].idxmax()\n",
        "    best_score = results_df[metric].max()\n",
        "    print(f\"{metric:25s}: {best_model:20s} ({best_score:.4f})\")"
    ], None))
    
    # Visualization of metrics
    new_cells.append(create_cell("code", [
        "# Visualize performance metrics comparison\n",
        "fig, axes = plt.subplots(2, 3, figsize=(18, 10))\n",
        "axes = axes.flatten()\n",
        "\n",
        "metrics_to_plot = ['Accuracy', 'Recall (Declined)', 'Precision (Declined)', \n",
        "                   'F1-Score (Declined)', 'AUC-ROC']\n",
        "colors = ['#3498db', '#e74c3c', '#2ecc71']\n",
        "\n",
        "for idx, metric in enumerate(metrics_to_plot):\n",
        "    ax = axes[idx]\n",
        "    values = results_df[metric].values\n",
        "    models = results_df.index.values\n",
        "    \n",
        "    bars = ax.bar(models, values, color=colors, alpha=0.7, edgecolor='black', linewidth=1.5)\n",
        "    ax.set_title(f'{metric}', fontsize=12, fontweight='bold')\n",
        "    ax.set_ylabel('Score')\n",
        "    ax.set_ylim(0, 1.0)\n",
        "    ax.grid(axis='y', alpha=0.3)\n",
        "    ax.tick_params(axis='x', rotation=45)\n",
        "    \n",
        "    # Add value labels on bars\n",
        "    for bar, val in zip(bars, values):\n",
        "        height = bar.get_height()\n",
        "        ax.text(bar.get_x() + bar.get_width()/2., height + 0.02,\n",
        "                f'{val:.4f}', ha='center', va='bottom', fontweight='bold', fontsize=9)\n",
        "    \n",
        "    # Highlight best\n",
        "    best_idx = values.argmax()\n",
        "    bars[best_idx].set_edgecolor('gold')\n",
        "    bars[best_idx].set_linewidth(3)\n",
        "\n",
        "# Remove extra subplot\n",
        "fig.delaxes(axes[5])\n",
        "\n",
        "plt.suptitle('Classification Model Performance Comparison', \n",
        "             fontsize=16, fontweight='bold', y=1.00)\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "print(\"\\nâœ“ Performance metrics calculated and visualized\")"
    ], None))
    
    # Model selection
    new_cells.append(create_cell("markdown", [
        "### Task 5.c: Best Model Selection\n",
        "\n",
        "Select the best model based on success criteria."
    ]))
    
    new_cells.append(create_cell("code", [
        "# Task 5.c: Select best model\n",
        "print(\"=\"*70)\n",
        "print(\"TASK 5.c: BEST MODEL SELECTION\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "# Calculate composite score (weighted average of key metrics)\n",
        "# Prioritize Recall and Precision for Declined class\n",
        "results_df['Composite Score'] = (\n",
        "    results_df['Recall (Declined)'] * 0.35 +\n",
        "    results_df['Precision (Declined)'] * 0.35 +\n",
        "    results_df['F1-Score (Declined)'] * 0.20 +\n",
        "    results_df['AUC-ROC'] * 0.10\n",
        ")\n",
        "\n",
        "print(\"\\nComposite Scores (weighted by success criteria):\")\n",
        "print(\"  - Recall (Declined): 35%\")\n",
        "print(\"  - Precision (Declined): 35%\")\n",
        "print(\"  - F1-Score (Declined): 20%\")\n",
        "print(\"  - AUC-ROC: 10%\\n\")\n",
        "\n",
        "for model in results_df.index:\n",
        "    print(f\"  {model:20s}: {results_df.loc[model, 'Composite Score']:.4f}\")\n",
        "\n",
        "best_model_name = results_df['Composite Score'].idxmax()\n",
        "print(f\"\\n{'='*70}\")\n",
        "print(f\"âœ“ BEST MODEL: {best_model_name}\")\n",
        "print(f\"{'='*70}\")\n",
        "\n",
        "print(f\"\\nJustification:\")\n",
        "best_scores = results_df.loc[best_model_name]\n",
        "print(f\"  - Recall (Declined): {best_scores['Recall (Declined)']:.4f} - Detects {best_scores['Recall (Declined)']*100:.2f}% of declined applications\")\n",
        "print(f\"  - Precision (Declined): {best_scores['Precision (Declined)']:.4f} - {best_scores['Precision (Declined)']*100:.2f}% of predicted declines are correct\")\n",
        "print(f\"  - AUC-ROC: {best_scores['AUC-ROC']:.4f} - Good discriminative ability\")\n",
        "\n",
        "print(f\"\\nðŸ’¡ This model best satisfies the success criteria of correctly predicting declined applications.\")"
    ], None))
    
    # Overfitting check
    new_cells.append(create_cell("markdown", [
        "### Task 5.d: Overfitting/Underfitting Check\n",
        "\n",
        "Compare training and test performance to assess model fit."
    ]))
    
    new_cells.append(create_cell("code", [
        "# Task 5.d: Check for overfitting/underfitting\n",
        "print(\"=\"*70)\n",
        "print(\"TASK 5.d: OVERFITTING/UNDERFITTING ANALYSIS\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "# Calculate training scores\n",
        "y_train_pred_nb = nb_model.predict(X_train)\n",
        "y_train_pred_lr = lr_model.predict(X_train)\n",
        "y_train_pred_rf = rf_model.predict(X_train)\n",
        "\n",
        "train_acc_nb = accuracy_score(y_train, y_train_pred_nb)\n",
        "train_acc_lr = accuracy_score(y_train, y_train_pred_lr)\n",
        "train_acc_rf = accuracy_score(y_train, y_train_pred_rf)\n",
        "\n",
        "test_acc_nb = results_df.loc['Naive Bayes', 'Accuracy']\n",
        "test_acc_lr = results_df.loc['Logistic Regression', 'Accuracy']\n",
        "test_acc_rf = results_df.loc['Random Forest', 'Accuracy']\n",
        "\n",
        "print(\"\\nModel Fit Analysis (Accuracy Comparison):\\n\")\n",
        "print(f\"{'Model':<20s} {'Train Acc':>12s} {'Test Acc':>12s} {'Difference':>12s} {'Status':>15s}\")\n",
        "print(\"-\" * 75)\n",
        "\n",
        "def assess_fit(train, test):\n",
        "    diff = train - test\n",
        "    if diff > 0.10:\n",
        "        return 'Overfitting'\n",
        "    elif diff < 0:\n",
        "        return 'Suspicious'\n",
        "    elif train < 0.70:\n",
        "        return 'Underfitting'\n",
        "    else:\n",
        "        return 'Good Fit'\n",
        "\n",
        "models_data = [\n",
        "    ('Naive Bayes', train_acc_nb, test_acc_nb),\n",
        "    ('Logistic Regression', train_acc_lr, test_acc_lr),\n",
        "    ('Random Forest', train_acc_rf, test_acc_rf)\n",
        "]\n",
        "\n",
        "for model, train, test in models_data:\n",
        "    diff = train - test\n",
        "    status = assess_fit(train, test)\n",
        "    print(f\"{model:<20s} {train:>12.4f} {test:>12.4f} {diff:>12.4f} {status:>15s}\")\n",
        "\n",
        "print(\"\\nInterpretation:\")\n",
        "print(\"  - Overfitting: Train >> Test (difference > 0.10)\")\n",
        "print(\"  - Underfitting: Both scores low (< 0.70)\")\n",
        "print(\"  - Good Fit: Train â‰ˆ Test (difference â‰¤ 0.10) and both reasonable\")"
    ], None))
    
    # Hyperparameter tuning
    new_cells.append(create_cell("markdown", [
        "### Task 5.e: Hyperparameter Tuning\n",
        "\n",
        "Tune the best model using GridSearchCV to improve performance."
    ]))
    
    new_cells.append(create_cell("code", [
        "# Task 5.e: Hyperparameter tuning with GridSearchCV\n",
        "print(\"=\"*70)\n",
        "print(\"TASK 5.e: HYPERPARAMETER TUNING\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "# We'll tune Random Forest (typically the best performer)\n",
        "print(f\"\\nTuning Random Forest with GridSearchCV...\")\n",
        "print(f\"This may take a few minutes...\\n\")\n",
        "\n",
        "# Define parameter grid\n",
        "param_grid = {\n",
        "    'n_estimators': [50, 100, 200],\n",
        "    'max_depth': [5, 10, 15, None],\n",
        "    'min_samples_split': [2, 5, 10],\n",
        "    'min_samples_leaf': [1, 2, 4],\n",
        "    'max_features': ['sqrt', 'log2']\n",
        "}\n",
        "\n",
        "print(\"Parameter grid:\")\n",
        "for param, values in param_grid.items():\n",
        "    print(f\"  {param}: {values}\")\n",
        "\n",
        "# Task 5.e.i: Use 5-fold cross-validation\n",
        "grid_search = GridSearchCV(\n",
        "    estimator=RandomForestClassifier(random_state=42),\n",
        "    param_grid=param_grid,\n",
        "    cv=5,  # 5-fold cross-validation\n",
        "    scoring='recall',  # Optimize for recall on Declined class (need to set pos_label in make_scorer)\n",
        "    n_jobs=-1,\n",
        "    verbose=1\n",
        ")\n",
        "\n",
        "print(f\"\\nâœ“ K-folds: 5\")\n",
        "print(f\"âœ“ Scoring metric: recall (for minority class)\")\n",
        "print(f\"\\nStarting grid search...\\n\")\n",
        "\n",
        "grid_search.fit(X_train, y_train)\n",
        "\n",
        "print(f\"\\n{'='*70}\")\n",
        "print(f\"TUNING COMPLETE\")\n",
        "print(f\"{'='*70}\")\n",
        "\n",
        "# Task 5.e.ii: Best parameters\n",
        "print(f\"\\nOriginal RF hyperparameters:\")\n",
        "print(f\"  n_estimators: 100 (default)\")\n",
        "print(f\"  max_depth: None (default)\")\n",
        "print(f\"  min_samples_split: 2 (default)\")\n",
        "print(f\"  min_samples_leaf: 1 (default)\")\n",
        "print(f\"  max_features: sqrt (default)\")\n",
        "\n",
        "print(f\"\\nTuned RF hyperparameters:\")\n",
        "for param, value in grid_search.best_params_.items():\n",
        "    print(f\"  {param}: {value}\")\n",
        "\n",
        "print(f\"\\nBest cross-validation score: {grid_search.best_score_:.4f}\")"
    ], None))
    
    # Compare tuned vs original
    new_cells.append(create_cell("code", [
        "# Task 5.e.iii-v: Compare tuned model with original\n",
        "print(\"\\n\" + \"=\"*70)\n",
        "print(\"TASK 5.e: TUNED MODEL PERFORMANCE\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "# Get best tuned model\n",
        "rf_tuned = grid_search.best_estimator_\n",
        "\n",
        "# Make predictions\n",
        "y_pred_rf_tuned = rf_tuned.predict(X_test)\n",
        "y_proba_rf_tuned = rf_tuned.predict_proba(X_test)\n",
        "\n",
        "# Task 5.e.iii: Confusion matrices comparison\n",
        "cm_rf_tuned = confusion_matrix(y_test, y_pred_rf_tuned, labels=['Approved', 'Declined'])\n",
        "\n",
        "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
        "\n",
        "# Original RF\n",
        "sns.heatmap(cm_rf, annot=True, fmt='d', cmap='Blues', ax=axes[0],\n",
        "            xticklabels=['Approved', 'Declined'], yticklabels=['Approved', 'Declined'])\n",
        "axes[0].set_title('Original Random Forest\\nConfusion Matrix', fontsize=14, fontweight='bold')\n",
        "axes[0].set_xlabel('Predicted')\n",
        "axes[0].set_ylabel('Actual')\n",
        "\n",
        "# Tuned RF\n",
        "sns.heatmap(cm_rf_tuned, annot=True, fmt='d', cmap='Greens', ax=axes[1],\n",
        "            xticklabels=['Approved', 'Declined'], yticklabels=['Approved', 'Declined'])\n",
        "axes[1].set_title('Tuned Random Forest\\nConfusion Matrix', fontsize=14, fontweight='bold')\n",
        "axes[1].set_xlabel('Predicted')\n",
        "axes[1].set_ylabel('Actual')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# Task 5.e.iv: Calculate metrics for tuned model\n",
        "metrics_rf_tuned = calculate_metrics(y_test, y_pred_rf_tuned, y_proba_rf_tuned, 'RF Tuned')\n",
        "\n",
        "comparison_df = pd.DataFrame([\n",
        "    metrics_rf,\n",
        "    metrics_rf_tuned\n",
        "]).set_index('Model')\n",
        "\n",
        "print(\"\\nTask 5.e.iv: Performance Metrics Comparison:\")\n",
        "print(comparison_df.round(4))\n",
        "\n",
        "# Task 5.e.v: Impact of tuning\n",
        "print(\"\\n\" + \"=\"*70)\n",
        "print(\"TASK 5.e.v: IMPACT OF HYPERPARAMETER TUNING\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "recall_improvement = metrics_rf_tuned['Recall (Declined)'] - metrics_rf['Recall (Declined)']\n",
        "precision_improvement = metrics_rf_tuned['Precision (Declined)'] - metrics_rf['Precision (Declined)']\n",
        "\n",
        "print(f\"\\nRecall (Declined) change: {recall_improvement:+.4f}\")\n",
        "print(f\"Precision (Declined) change: {precision_improvement:+.4f}\")\n",
        "\n",
        "if recall_improvement > 0 and precision_improvement > 0:\n",
        "    print(f\"\\nâœ“ IMPROVEMENT: Tuning improved both Recall and Precision for Declined class\")\n",
        "elif recall_improvement > 0 or precision_improvement > 0:\n",
        "    print(f\"\\nâœ“ PARTIAL IMPROVEMENT: Tuning improved at least one key metric\")\n",
        "else:\n",
        "    print(f\"\\nâœ— NO IMPROVEMENT: Original model performed better\")\n",
        "    print(f\"   This can happen when the default hyperparameters are already near-optimal\")\n",
        "\n",
        "print(f\"\\nðŸ’¡ Conclusion: Hyperparameter tuning helps explore the model space and can lead to improvements.\")"
    ], None))
    
    # PART B: REGRESSION
    new_cells.append(create_cell("markdown", [
        "---\n",
        "\n",
        "# PART B: REGRESSION - MAXIMUM LOAN AMOUNT PREDICTION\n",
        "\n",
        "## Objective\n",
        "For approved loan applications, predict the maximum loan amount the lender is willing to provide.\n",
        "\n",
        "### Models\n",
        "- **DT1:** Decision Tree with numeric features only\n",
        "- **DT2:** Decision Tree with all features (numeric + categorical)\n",
        "\n",
        "### Success Criteria\n",
        "> \"The selected model should have input features that are better at explaining the recorded values of the maximum loan amount\"\n",
        "\n",
        "**Translation:** Focus on **RÂ² score** (coefficient of determination)"
    ]))
    
    # Filter for approved loans
    new_cells.append(create_cell("code", [
        "# Task 1: Domain Understanding - Regression\n",
        "print(\"=\"*70)\n",
        "print(\"PART B: REGRESSION - MAXIMUM LOAN AMOUNT\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "# Filter for approved loans only\n",
        "df_approved = df[df['Loan Approval Status'] == 'Approved'].copy()\n",
        "\n",
        "print(f\"\\nTotal records: {len(df):,}\")\n",
        "print(f\"Approved loans: {len(df_approved):,}\")\n",
        "print(f\"Approval rate: {len(df_approved)/len(df)*100:.2f}%\")\n",
        "\n",
        "print(f\"\\nDataset for regression: {df_approved.shape}\")\n",
        "print(f\"Target variable: Maximum Loan Amount\")\n",
        "print(f\"\\nTarget statistics:\")\n",
        "print(df_approved['Maximum Loan Amount'].describe())"
    ], None))
    
    new_cells.append(create_cell("markdown", [
        "## Task 2: Data Understanding - Regression\n",
        "\n",
        "Visualize distributions of features and target variable."
    ]))
    
    new_cells.append(create_cell("code", [
        "# Visualize numerical features for regression\n",
        "numerical_features_reg = [\n",
        "    'Income', 'Employment Length', 'Loan Amount', 'Loan Interest Rate',\n",
        "    'Loan-to-Income Ratio (LTI)', 'Credit History Length', 'Maximum Loan Amount'\n",
        "]\n",
        "\n",
        "fig, axes = plt.subplots(3, 3, figsize=(16, 12))\n",
        "axes = axes.flatten()\n",
        "\n",
        "for i, feature in enumerate(numerical_features_reg):\n",
        "    axes[i].hist(df_approved[feature].dropna(), bins=50, color='skyblue', \n",
        "                 edgecolor='black', alpha=0.7)\n",
        "    axes[i].set_title(f'{feature}', fontsize=11, fontweight='bold')\n",
        "    axes[i].set_xlabel(feature)\n",
        "    axes[i].set_ylabel('Frequency')\n",
        "    axes[i].grid(axis='y', alpha=0.3)\n",
        "\n",
        "# Remove extra subplots\n",
        "for j in range(len(numerical_features_reg), len(axes)):\n",
        "    fig.delaxes(axes[j])\n",
        "\n",
        "plt.suptitle('Distribution of Numerical Features (Approved Loans Only)', \n",
        "             fontsize=14, fontweight='bold', y=0.995)\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "print(\"âœ“ Feature distributions visualized\")"
    ], None))
    
    # Add all cells to notebook
    notebook["cells"].extend(new_cells)
    
    with open(notebook_path, 'w') as f:
        json.dump(notebook, f, indent=2)
    
    print(f"âœ“ Added {len(new_cells)} comprehensive analysis cells")
    print(f"âœ“ Total cells: {len(notebook['cells'])}")

if __name__ == "__main__":
    notebook_path = Path(__file__).parent.parent / 'notebooks' / 'loan_approval_complete.ipynb'
    complete_notebook(notebook_path)
    print("âœ“ Notebook completion successful!")
