{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loan Approval Coursework - Machine Learning Analysis\n",
    "\n",
    "## Part A: Loan Approval Status Prediction (Classification)\n",
    "## Part B: Maximum Loan Amount Prediction (Regression)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Required Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from pathlib import Path\n",
    "\n",
    "# Classification models\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# Regression models\n",
    "from sklearn.tree import DecisionTreeRegressor, plot_tree\n",
    "\n",
    "# Model evaluation\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score, recall_score, precision_score, f1_score, roc_auc_score\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Warnings\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the dataset\n",
    "DATA_PATH = Path('../data/loan_approval_data.csv')\n",
    "\n",
    "df = pd.read_csv(DATA_PATH, low_memory=False)\n",
    "print(f\"Dataset shape: {df.shape}\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standardise column names for readability\n",
    "rename_map = {\n",
    "    'id': 'ID',\n",
    "    'age': 'Age',\n",
    "    'Sex': 'Sex',\n",
    "    'Education_Qualifications': 'Education Qualifications',\n",
    "    'income': 'Income',\n",
    "    'home_ownership': 'Home Ownership',\n",
    "    'emplyment_length': 'Employment Length',\n",
    "    'loan_intent': 'Loan Intent',\n",
    "    'loan_amount': 'Loan Amount',\n",
    "    'loan_interest_rate': 'Loan Interest Rate',\n",
    "    'loan_income_ratio': 'Loan-to-Income Ratio (LTI)',\n",
    "    'payment_default_on_file': 'Payment Default on File',\n",
    "    'credit_history_length': 'Credit History Length',\n",
    "    'loan_approval_status': 'Loan Approval Status',\n",
    "    'max_allowed_loan': 'Maximum Loan Amount',\n",
    "    'Credit_Application_Acceptance': 'Credit Application Acceptance'\n",
    "}\n",
    "\n",
    "df = df.rename(columns=rename_map)\n",
    "\n",
    "# Preview renamed columns\n",
    "print(df.columns.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data quality overview for retained variables and target\n",
    "print(\"Missing values per column:\\n\")\n",
    "print(df[retained_vars].isnull().sum())\n",
    "\n",
    "print(\"\\nDuplicate rows in retained set:\")\n",
    "print(df[retained_vars].duplicated().sum())\n",
    "\n",
    "print(\"\\nUnique values in Loan Approval Status:\")\n",
    "print(df['Loan Approval Status'].value_counts(dropna=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 2: Data Understanding\n",
    "\n",
    "retained_vars = [\n",
    "    'Education Qualifications', 'Income', 'Home Ownership', 'Employment Length',\n",
    "    'Loan Intent', 'Loan Amount', 'Loan Interest Rate', 'Loan-to-Income Ratio (LTI)',\n",
    "    'Payment Default on File', 'Credit History Length', 'Loan Approval Status',\n",
    "    'Maximum Loan Amount'\n",
    "]\n",
    "\n",
    "df_retained = df[retained_vars].copy()\n",
    "print(\"Basic statistics for retained variables:\\n\")\n",
    "print(df_retained.describe(include='all'))\n",
    "print(\"\\nVariable Types:\\n\")\n",
    "print(df_retained.dtypes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 2: Data Understanding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Statistical description of retained variables\n",
    "retained_vars = ['Education Qualifications', 'Income', 'Home Ownership', 'Employment Length',\n",
    "                 'Loan Intent', 'Loan Amount', 'Loan Interest Rate', 'Loan-to-Income Ratio (LTI)',\n",
    "                 'Payment Default on File', 'Credit History Length', 'Loan Approval Status',\n",
    "                 'Maximum Loan Amount']\n",
    "\n",
    "df_retained = df[retained_vars].copy()\n",
    "print(df_retained.describe())\n",
    "print(\"\\nVariable Types:\")\n",
    "print(df_retained.dtypes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Task 3: Data Preparation\n",
    "\n",
    "# Explore data quality issues\n",
    "print(\"Missing values across retained variables:\\n\")\n",
    "print(df[retained_vars].isnull().sum())\n",
    "\n",
    "print(\"\\nDuplicate rows considering retained variables:\")\n",
    "print(df[retained_vars].duplicated().sum())\n",
    "\n",
    "print(\"\\nLoan Approval Status raw categories:\\n\")\n",
    "print(df['Loan Approval Status'].value_counts(dropna=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TASK 3.b: Standardise Payment Default on File to binary values\n",
    "\n",
    "print(\"Before cleaning - Payment Default on File:\")\n",
    "print(df['Payment Default on File'].value_counts(dropna=False))\n",
    "\n",
    "# Standardise to 'Y' and 'N' only\n",
    "df['Payment Default on File'] = df['Payment Default on File'].replace({\n",
    "    'YES': 'Y',\n",
    "    'NO': 'N'\n",
    "})\n",
    "\n",
    "# Impute missing values with mode (most frequent value)\n",
    "mode_default = df['Payment Default on File'].mode()[0]\n",
    "df['Payment Default on File'].fillna(mode_default, inplace=True)\n",
    "\n",
    "print(\"\\nAfter cleaning - Payment Default on File:\")\n",
    "print(df['Payment Default on File'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TASK 3.c: Handle missing values in Loan Interest Rate\n",
    "\n",
    "print(f\"Missing values in Loan Interest Rate: {df['Loan Interest Rate'].isnull().sum()}\")\n",
    "\n",
    "# Impute with median (robust to outliers)\n",
    "median_rate = df['Loan Interest Rate'].median()\n",
    "df['Loan Interest Rate'].fillna(median_rate, inplace=True)\n",
    "\n",
    "print(f\"After imputation: {df['Loan Interest Rate'].isnull().sum()} missing values\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TASK 3.d: Verify no missing values remain in retained variables\n",
    "\n",
    "retained_vars = [\n",
    "    'Education Qualifications', 'Income', 'Home Ownership', 'Employment Length',\n",
    "    'Loan Intent', 'Loan Amount', 'Loan Interest Rate', 'Loan-to-Income Ratio (LTI)',\n",
    "    'Payment Default on File', 'Credit History Length', 'Loan Approval Status',\n",
    "    'Maximum Loan Amount'\n",
    "]\n",
    "\n",
    "print(\"Final check - Missing values in retained variables:\")\n",
    "print(df[retained_vars].isnull().sum())\n",
    "print(f\"\\nDataset shape after cleaning: {df.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 4: Modelling - Classification\n",
    "\n",
    "# TASK 4.a: Algorithm details table\n",
    "# This information will be used in the report:\n",
    "# Algorithm | Type | Learnable Parameters | Hyperparameters | Package\n",
    "# NB | Non-parametric | Class priors, feature means/variances | var_smoothing | sklearn.naive_bayes.GaussianNB\n",
    "# LR | Parametric | Coefficients (weights), intercept | C, penalty, solver | sklearn.linear_model.LogisticRegression\n",
    "# RF | Non-parametric | Split rules at nodes | n_estimators, max_depth, min_samples_split | sklearn.ensemble.RandomForestClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TASK 4.b: Prepare categorical features only for classification\n",
    "\n",
    "# Select categorical features from retained variables\n",
    "categorical_features = [\n",
    "    'Education Qualifications',\n",
    "    'Home Ownership',\n",
    "    'Loan Intent',\n",
    "    'Payment Default on File'\n",
    "]\n",
    "\n",
    "# Create feature matrix X with categorical features only\n",
    "X_cat = df[categorical_features].copy()\n",
    "\n",
    "# Target variable\n",
    "y = df['Loan Approval Status'].copy()\n",
    "\n",
    "# One-hot encode categorical features\n",
    "X_encoded = pd.get_dummies(X_cat, drop_first=True)\n",
    "\n",
    "print(\"Feature names used for classification:\")\n",
    "print(X_encoded.columns.tolist())\n",
    "print(f\"\\nFeature matrix shape: {X_encoded.shape}\")\n",
    "print(f\"Target variable shape: {y.shape}\")\n",
    "print(f\"\\nTarget distribution:\\n{y.value_counts()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TASK 4.b.ii: Train-test split with 80:20 ratio\n",
    "# Justification: 80:20 is a standard split that provides sufficient training data \n",
    "# while preserving enough test data for reliable evaluation (Géron, 2019).\n",
    "\n",
    "# TASK 4.b.iv: Ensure reproducibility and stratification\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X_encoded, \n",
    "    y, \n",
    "    test_size=0.2,           # 80:20 split\n",
    "    random_state=42,          # Ensures reproducibility - same split every time\n",
    "    stratify=y                # Maintains class proportions in train and test\n",
    ")\n",
    "\n",
    "print(\"Training set shape:\", X_train.shape)\n",
    "print(\"Test set shape:\", X_test.shape)\n",
    "print(\"\\nTraining set class distribution:\")\n",
    "print(y_train.value_counts(normalize=True))\n",
    "print(\"\\nTest set class distribution:\")\n",
    "print(y_test.value_counts(normalize=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build and train three classification models\n",
    "\n",
    "# 1. Naive Bayes\n",
    "nb_model = GaussianNB()\n",
    "nb_model.fit(X_train, y_train)\n",
    "print(\"Naive Bayes model trained successfully\")\n",
    "\n",
    "# 2. Logistic Regression\n",
    "lr_model = LogisticRegression(random_state=42, max_iter=1000)\n",
    "lr_model.fit(X_train, y_train)\n",
    "print(\"Logistic Regression model trained successfully\")\n",
    "\n",
    "# 3. Random Forest\n",
    "rf_model = RandomForestClassifier(random_state=42)\n",
    "rf_model.fit(X_train, y_train)\n",
    "print(\"Random Forest model trained successfully\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 5: Evaluation - Classification\n",
    "\n",
    "# TASK 5.a: Generate predictions and confusion matrices for all models\n",
    "\n",
    "# Naive Bayes predictions\n",
    "y_pred_nb = nb_model.predict(X_test)\n",
    "cm_nb = confusion_matrix(y_test, y_pred_nb)\n",
    "\n",
    "print(\"Naive Bayes - Test Confusion Matrix:\")\n",
    "print(cm_nb)\n",
    "print(\"\\nClass labels:\", nb_model.classes_)\n",
    "\n",
    "# Logistic Regression predictions\n",
    "y_pred_lr = lr_model.predict(X_test)\n",
    "cm_lr = confusion_matrix(y_test, y_pred_lr)\n",
    "\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"Logistic Regression - Test Confusion Matrix:\")\n",
    "print(cm_lr)\n",
    "\n",
    "# Random Forest predictions\n",
    "y_pred_rf = rf_model.predict(X_test)\n",
    "cm_rf = confusion_matrix(y_test, y_pred_rf)\n",
    "\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"Random Forest - Test Confusion Matrix:\")\n",
    "print(cm_rf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TASK 5.b: Calculate test performance metrics for all models\n",
    "\n",
    "# Helper function to calculate all metrics\n",
    "def calculate_metrics(y_true, y_pred, y_proba=None):\n",
    "    \"\"\"Calculate classification metrics\"\"\"\n",
    "    metrics = {\n",
    "        'Accuracy': accuracy_score(y_true, y_pred),\n",
    "        'Recall': recall_score(y_true, y_pred, pos_label='Declined'),  # Focus on Declined\n",
    "        'Precision': precision_score(y_true, y_pred, pos_label='Declined'),\n",
    "        'F-Score': f1_score(y_true, y_pred, pos_label='Declined')\n",
    "    }\n",
    "    \n",
    "    # AUC-ROC requires probability scores\n",
    "    if y_proba is not None:\n",
    "        # Convert to binary (Declined=1, Approved=0)\n",
    "        y_binary = (y_true == 'Declined').astype(int)\n",
    "        metrics['AUC-ROC'] = roc_auc_score(y_binary, y_proba[:, 1])\n",
    "    \n",
    "    return metrics\n",
    "\n",
    "# Calculate for Naive Bayes\n",
    "y_proba_nb = nb_model.predict_proba(X_test)\n",
    "metrics_nb = calculate_metrics(y_test, y_pred_nb, y_proba_nb)\n",
    "\n",
    "# Calculate for Logistic Regression\n",
    "y_proba_lr = lr_model.predict_proba(X_test)\n",
    "metrics_lr = calculate_metrics(y_test, y_pred_lr, y_proba_lr)\n",
    "\n",
    "# Calculate for Random Forest\n",
    "y_proba_rf = rf_model.predict_proba(X_test)\n",
    "metrics_rf = calculate_metrics(y_test, y_pred_rf, y_proba_rf)\n",
    "\n",
    "# Display results in a table\n",
    "results_df = pd.DataFrame({\n",
    "    'NB': metrics_nb,\n",
    "    'LR': metrics_lr,\n",
    "    'RF': metrics_rf\n",
    "})\n",
    "\n",
    "print(\"Test Performance Metrics (All Models):\")\n",
    "print(results_df.round(4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TASK 5.c: Identify best model based on success criteria\n",
    "# Success criteria: High Recall and Precision for \"Declined\" class\n",
    "\n",
    "print(\"Based on the metrics:\")\n",
    "print(\"- USE: Recall, Precision, F-Score (focused on Declined class)\")\n",
    "print(\"- USE: AUC-ROC (overall discriminative ability)\")\n",
    "print(\"- DO NOT USE: Accuracy alone (due to class imbalance)\")\n",
    "print(\"\\nBest model selection will prioritize Recall and Precision for Declined predictions.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TASK 5.d: Check for overfitting/underfitting\n",
    "# Compare training and test performance for each model\n",
    "\n",
    "print(\"Checking model fit (Training vs Test scores):\\n\")\n",
    "\n",
    "# Naive Bayes\n",
    "y_train_pred_nb = nb_model.predict(X_train)\n",
    "train_acc_nb = accuracy_score(y_train, y_train_pred_nb)\n",
    "test_acc_nb = accuracy_score(y_test, y_pred_nb)\n",
    "print(f\"Naive Bayes - Train Accuracy: {train_acc_nb:.4f}, Test Accuracy: {test_acc_nb:.4f}\")\n",
    "\n",
    "# Logistic Regression\n",
    "y_train_pred_lr = lr_model.predict(X_train)\n",
    "train_acc_lr = accuracy_score(y_train, y_train_pred_lr)\n",
    "test_acc_lr = accuracy_score(y_test, y_pred_lr)\n",
    "print(f\"Logistic Regression - Train Accuracy: {train_acc_lr:.4f}, Test Accuracy: {test_acc_lr:.4f}\")\n",
    "\n",
    "# Random Forest\n",
    "y_train_pred_rf = rf_model.predict(X_train)\n",
    "train_acc_rf = accuracy_score(y_train, y_train_pred_rf)\n",
    "test_acc_rf = accuracy_score(y_test, y_pred_rf)\n",
    "print(f\"Random Forest - Train Accuracy: {train_acc_rf:.4f}, Test Accuracy: {test_acc_rf:.4f}\")\n",
    "\n",
    "print(\"\\nInterpretation:\")\n",
    "print(\"- If train >> test: Overfitting (model memorizes training data)\")\n",
    "print(\"- If train ≈ test and both low: Underfitting (model too simple)\")\n",
    "print(\"- If train ≈ test and both high: Good fit\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TASK 5.e: Hyperparameter tuning with GridSearchCV\n",
    "# Tuning Random Forest (assuming it's the best model)\n",
    "\n",
    "print(\"Performing hyperparameter tuning on Random Forest...\\n\")\n",
    "\n",
    "# Define parameter grid\n",
    "param_grid = {\n",
    "    'n_estimators': [50, 100, 200],\n",
    "    'max_depth': [5, 10, 15, None],\n",
    "    'min_samples_split': [2, 5, 10],\n",
    "    'min_samples_leaf': [1, 2, 4]\n",
    "}\n",
    "\n",
    "# TASK 5.e.i: GridSearchCV with 5-fold cross-validation\n",
    "grid_search = GridSearchCV(\n",
    "    estimator=RandomForestClassifier(random_state=42),\n",
    "    param_grid=param_grid,\n",
    "    cv=5,  # 5-fold cross-validation\n",
    "    scoring='recall',  # Optimize for recall on Declined class\n",
    "    n_jobs=-1,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "print(\"K-folds used: 5\")\n",
    "print(\"This will take a moment...\\n\")\n",
    "\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "print(\"\\nTuning complete!\")\n",
    "print(f\"Best parameters: {grid_search.best_params_}\")\n",
    "print(f\"Best cross-validation score: {grid_search.best_score_:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TASK 5.e.ii-v: Compare tuned model with original\n",
    "\n",
    "# Get best tuned model\n",
    "rf_tuned = grid_search.best_estimator_\n",
    "\n",
    "# Predictions from tuned model\n",
    "y_pred_rf_tuned = rf_tuned.predict(X_test)\n",
    "cm_rf_tuned = confusion_matrix(y_test, y_pred_rf_tuned)\n",
    "\n",
    "print(\"TASK 5.e.ii: Original vs Tuned Hyperparameters\")\n",
    "print(\"=\"*60)\n",
    "print(\"Original RF hyperparameters:\")\n",
    "print(f\"  n_estimators: 100 (default)\")\n",
    "print(f\"  max_depth: None (default)\")\n",
    "print(f\"  min_samples_split: 2 (default)\")\n",
    "print(f\"  min_samples_leaf: 1 (default)\")\n",
    "print(\"\\nTuned RF hyperparameters:\")\n",
    "for param, value in grid_search.best_params_.items():\n",
    "    print(f\"  {param}: {value}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"TASK 5.e.iii: Confusion Matrix Comparison\")\n",
    "print(\"=\"*60)\n",
    "print(\"Original RF Confusion Matrix:\")\n",
    "print(cm_rf)\n",
    "print(\"\\nTuned RF Confusion Matrix:\")\n",
    "print(cm_rf_tuned)\n",
    "\n",
    "# Calculate metrics for tuned model\n",
    "y_proba_rf_tuned = rf_tuned.predict_proba(X_test)\n",
    "metrics_rf_tuned = calculate_metrics(y_test, y_pred_rf_tuned, y_proba_rf_tuned)\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"TASK 5.e.iv: Performance Metrics Comparison\")\n",
    "print(\"=\"*60)\n",
    "comparison_df = pd.DataFrame({\n",
    "    'Original RF': metrics_rf,\n",
    "    'Tuned RF': metrics_rf_tuned\n",
    "})\n",
    "print(comparison_df.round(4))\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"TASK 5.e.v: Impact of Tuning\")\n",
    "print(\"=\"*60)\n",
    "if metrics_rf_tuned['Recall'] > metrics_rf['Recall']:\n",
    "    print(\"✓ Tuning IMPROVED the model's ability to detect Declined applications\")\n",
    "elif metrics_rf_tuned['Recall'] < metrics_rf['Recall']:\n",
    "    print(\"✗ Tuning REDUCED the model's ability to detect Declined applications\")\n",
    "else:\n",
    "    print(\"= Tuning had NO CHANGE on Recall for Declined applications\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Distribution plots for regression variables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 1: Domain Understanding - Regression\n",
    "\n",
    "# Filter dataset for APPROVED loans only\n",
    "df_approved = df[df['Loan Approval Status'] == 'Approved'].copy()\n",
    "\n",
    "print(f\"Total records: {len(df)}\")\n",
    "print(f\"Approved loans: {len(df_approved)}\")\n",
    "print(f\"Percentage approved: {len(df_approved)/len(df)*100:.2f}%\")\n",
    "\n",
    "# Features for regression (exclude target and non-predictive variables)\n",
    "regression_features = [\n",
    "    'Education Qualifications', 'Income', 'Home Ownership', 'Employment Length',\n",
    "    'Loan Intent', 'Loan Amount', 'Loan Interest Rate', 'Loan-to-Income Ratio (LTI)',\n",
    "    'Payment Default on File', 'Credit History Length'\n",
    "]\n",
    "\n",
    "print(f\"\\nDimensions of regression dataset: {df_approved[regression_features + ['Maximum Loan Amount']].shape}\")\n",
    "print(\"\\nFeatures for regression modelling:\")\n",
    "for i, feat in enumerate(regression_features, 1):\n",
    "    print(f\"{i}. {feat}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Task 2: Data Understanding - Regression\n",
    "\n",
    "# Plot distributions of numerical features and target\n",
    "numerical_features = [\n",
    "    'Income', 'Employment Length', 'Loan Amount', 'Loan Interest Rate',\n",
    "    'Loan-to-Income Ratio (LTI)', 'Credit History Length', 'Maximum Loan Amount'\n",
    "]\n",
    "\n",
    "fig, axes = plt.subplots(3, 3, figsize=(15, 12))\n",
    "axes = axes.flatten()\n",
    "\n",
    "for i, feature in enumerate(numerical_features):\n",
    "    axes[i].hist(df_approved[feature].dropna(), bins=50, edgecolor='black')\n",
    "    axes[i].set_title(f'Distribution of {feature}')\n",
    "    axes[i].set_xlabel(feature)\n",
    "    axes[i].set_ylabel('Frequency')\n",
    "    axes[i].grid(alpha=0.3)\n",
    "\n",
    "# Remove extra subplots\n",
    "for j in range(len(numerical_features), len(axes)):\n",
    "    fig.delaxes(axes[j])\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"Summary statistics for regression features:\")\n",
    "print(df_approved[numerical_features].describe())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 3: Data Preprocessing - Regression\n",
    "\n",
    "# TASK 3.a: Investigate need for scaling\n",
    "\n",
    "print(\"Checking ranges and scales of numerical features:\\n\")\n",
    "scale_check = df_approved[numerical_features].describe().loc[['min', 'max', 'mean', 'std']]\n",
    "print(scale_check)\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"RECOMMENDATION:\")\n",
    "print(\"=\"*60)\n",
    "print(\"Decision Tree regressors are SCALE-INVARIANT (they use splits, not distances).\")\n",
    "print(\"Therefore, scaling is NOT required for this task.\")\n",
    "print(\"\\nHowever, if using distance-based algorithms (e.g., KNN, SVM),\")\n",
    "print(\"scaling would be essential due to the vastly different ranges:\")\n",
    "print(f\"  - Income ranges from {df_approved['Income'].min():,.0f} to {df_approved['Income'].max():,.0f}\")\n",
    "print(f\"  - LTI ratio ranges from {df_approved['Loan-to-Income Ratio (LTI)'].min():.2f} to {df_approved['Loan-to-Income Ratio (LTI)'].max():.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Task 4: Modelling - Regression\n",
    "\n",
    "# TASK 4.a: Why Decision Tree for financial prediction?\n",
    "print(\"Benefits of Decision Tree Regressor:\")\n",
    "print(\"- Interpretable: Easy to explain decisions to financial analysts\")\n",
    "print(\"- Non-linear: Captures complex relationships in loan data\")\n",
    "print(\"- No scaling needed: Works directly with different feature ranges\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TASK 4.b: Prepare features for two regression models\n",
    "\n",
    "# Model 1 (DT1): Numeric features only\n",
    "numeric_features_only = [\n",
    "    'Income', 'Employment Length', 'Loan Amount', 'Loan Interest Rate',\n",
    "    'Loan-to-Income Ratio (LTI)', 'Credit History Length'\n",
    "]\n",
    "\n",
    "X_dt1 = df_approved[numeric_features_only].copy()\n",
    "y_reg = df_approved['Maximum Loan Amount'].copy()\n",
    "\n",
    "print(\"DT1 - Numeric features only:\")\n",
    "print(f\"Features: {X_dt1.columns.tolist()}\")\n",
    "print(f\"Shape: {X_dt1.shape}\\n\")\n",
    "\n",
    "# Model 2 (DT2): All features (numeric + categorical encoded)\n",
    "categorical_reg_features = [\n",
    "    'Education Qualifications', 'Home Ownership', 'Loan Intent', 'Payment Default on File'\n",
    "]\n",
    "\n",
    "X_dt2_cat = df_approved[categorical_reg_features].copy()\n",
    "X_dt2_num = df_approved[numeric_features_only].copy()\n",
    "\n",
    "# One-hot encode categorical features\n",
    "X_dt2_cat_encoded = pd.get_dummies(X_dt2_cat, drop_first=True)\n",
    "\n",
    "# Combine numeric and encoded categorical\n",
    "X_dt2 = pd.concat([X_dt2_num, X_dt2_cat_encoded], axis=1)\n",
    "\n",
    "print(\"DT2 - All features (numeric + categorical):\")\n",
    "print(f\"Features: {X_dt2.columns.tolist()}\")\n",
    "print(f\"Shape: {X_dt2.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build and train Decision Tree regression models\n",
    "\n",
    "# DT1: Numeric features only\n",
    "dt1_model = DecisionTreeRegressor(random_state=42)\n",
    "dt1_model.fit(X_train_dt1, y_train_reg)\n",
    "print(\"DT1 model trained successfully (numeric features only)\")\n",
    "\n",
    "# DT2: All features\n",
    "dt2_model = DecisionTreeRegressor(random_state=42)\n",
    "dt2_model.fit(X_train_dt2, y_train_reg2)\n",
    "print(\"DT2 model trained successfully (all features)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 5: Evaluation - Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TASK 5.a: Calculate test performance metrics\n",
    "\n",
    "# Make predictions\n",
    "y_pred_dt1 = dt1_model.predict(X_test_dt1)\n",
    "y_pred_dt2 = dt2_model.predict(X_test_dt2)\n",
    "\n",
    "# Calculate metrics for DT1\n",
    "mse_dt1 = mean_squared_error(y_test_reg, y_pred_dt1)\n",
    "mae_dt1 = mean_absolute_error(y_test_reg, y_pred_dt1)\n",
    "r2_dt1 = r2_score(y_test_reg, y_pred_dt1)\n",
    "\n",
    "# Calculate metrics for DT2\n",
    "mse_dt2 = mean_squared_error(y_test_reg2, y_pred_dt2)\n",
    "mae_dt2 = mean_absolute_error(y_test_reg2, y_pred_dt2)\n",
    "r2_dt2 = r2_score(y_test_reg2, y_pred_dt2)\n",
    "\n",
    "# Display results\n",
    "regression_results = pd.DataFrame({\n",
    "    'Metric': ['MSE', 'MAE', 'R-Square'],\n",
    "    'DT1 (Numeric)': [mse_dt1, mae_dt1, r2_dt1],\n",
    "    'DT2 (All Features)': [mse_dt2, mae_dt2, r2_dt2]\n",
    "})\n",
    "\n",
    "print(\"TASK 5.a: Test Performance Metrics\")\n",
    "print(\"=\"*60)\n",
    "print(regression_results.to_string(index=False))\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"METRIC SELECTION:\")\n",
    "print(\"=\"*60)\n",
    "print(\"USE: R-Square - Explains how well features predict maximum loan amount\")\n",
    "print(\"DO NOT USE: MSE alone - Large values hard to interpret in original units\")\n",
    "print(\"DO NOT USE: MAE alone - Doesn't show proportion of variance explained\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TASK 5.b: Caveats of R-Square\n",
    "\n",
    "print(\"TASK 5.b: Caveats of R-Square Metric\")\n",
    "print(\"=\"*60)\n",
    "print(\"1. R² can be artificially inflated by adding more features\")\n",
    "print(\"2. R² doesn't indicate if predictions are biased (systematic errors)\")\n",
    "print(\"3. R² near 1.0 might indicate overfitting if train/test scores differ greatly\")\n",
    "print(\"4. R² alone doesn't reveal if residuals meet regression assumptions\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TASK 5.c: Select best model\n",
    "\n",
    "print(\"TASK 5.c: Best Model Selection\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "if r2_dt2 > r2_dt1:\n",
    "    print(f\"Best Model: DT2 (All Features)\")\n",
    "    print(f\"  R² Score: {r2_dt2:.4f}\")\n",
    "    print(f\"\\nJustification:\")\n",
    "    print(f\"DT2 has higher R², meaning it explains more variance in maximum loan amounts.\")\n",
    "    print(f\"Categorical features (Home Ownership, Loan Intent, etc.) add predictive value.\")\n",
    "    best_model = dt2_model\n",
    "    X_train_best = X_train_dt2\n",
    "    X_test_best = X_test_dt2\n",
    "    y_test_best = y_test_reg2\n",
    "    best_name = \"DT2\"\n",
    "else:\n",
    "    print(f\"Best Model: DT1 (Numeric Only)\")\n",
    "    print(f\"  R² Score: {r2_dt1:.4f}\")\n",
    "    print(f\"\\nJustification:\")\n",
    "    print(f\"DT1 performs as well as or better than DT2 with fewer features (simpler model).\")\n",
    "    best_model = dt1_model\n",
    "    X_train_best = X_train_dt1\n",
    "    X_test_best = X_test_dt1\n",
    "    y_test_best = y_test_reg\n",
    "    best_name = \"DT1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TASK 5.d: Rebuild best model with pre-pruning (max_depth=4)\n",
    "\n",
    "print(\"TASK 5.d: Pruning the Best Model\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Train pruned version\n",
    "if best_name == \"DT2\":\n",
    "    pruned_model = DecisionTreeRegressor(max_depth=4, random_state=42)\n",
    "    pruned_model.fit(X_train_dt2, y_train_reg2)\n",
    "    y_pred_pruned = pruned_model.predict(X_test_dt2)\n",
    "    r2_pruned = r2_score(y_test_reg2, y_pred_pruned)\n",
    "else:\n",
    "    pruned_model = DecisionTreeRegressor(max_depth=4, random_state=42)\n",
    "    pruned_model.fit(X_train_dt1, y_train_reg)\n",
    "    y_pred_pruned = pruned_model.predict(X_test_dt1)\n",
    "    r2_pruned = r2_score(y_test_reg, y_pred_pruned)\n",
    "\n",
    "print(f\"Original {best_name} R² Score: {r2_dt2 if best_name=='DT2' else r2_dt1:.4f}\")\n",
    "print(f\"Pruned {best_name} R² Score (max_depth=4): {r2_pruned:.4f}\")\n",
    "\n",
    "if r2_pruned < (r2_dt2 if best_name=='DT2' else r2_dt1):\n",
    "    print(\"\\nImpact: Pruning DECREASED performance (simpler but less accurate)\")\n",
    "else:\n",
    "    print(\"\\nImpact: Pruning MAINTAINED or IMPROVED performance (better generalization)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the pruned tree\n",
    "\n",
    "plt.figure(figsize=(20, 10))\n",
    "plot_tree(pruned_model, \n",
    "          feature_names=X_train_best.columns.tolist(),\n",
    "          filled=True,\n",
    "          rounded=True,\n",
    "          fontsize=10)\n",
    "plt.title(f'Pruned Decision Tree ({best_name}, max_depth=4)', fontsize=16)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"Tree structure is now limited to 4 levels for easier interpretation.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TASK 5.e: Predict maximum loan amount for client 60256\n",
    "\n",
    "print(\"TASK 5.e: Prediction for Client 60256\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Client details\n",
    "client_data = {\n",
    "    'Income': 57000,\n",
    "    'Employment Length': 15,\n",
    "    'Loan Amount': 25700,\n",
    "    'Loan Interest Rate': 23.0,\n",
    "    'Loan-to-Income Ratio (LTI)': 0.10,\n",
    "    'Credit History Length': 35\n",
    "}\n",
    "\n",
    "# If DT2 was best, add categorical features\n",
    "if best_name == \"DT2\":\n",
    "    client_data.update({\n",
    "        'Education Qualifications': 'Unknown',\n",
    "        'Home Ownership': 'Rent',\n",
    "        'Loan Intent': 'Medical',\n",
    "        'Payment Default on File': 'N'\n",
    "    })\n",
    "    \n",
    "    # Create DataFrame\n",
    "    client_df = pd.DataFrame([client_data])\n",
    "    \n",
    "    # Encode categorical features same way as training\n",
    "    client_cat = client_df[['Education Qualifications', 'Home Ownership', 'Loan Intent', 'Payment Default on File']]\n",
    "    client_num = client_df[numeric_features_only]\n",
    "    \n",
    "    client_cat_encoded = pd.get_dummies(client_cat, drop_first=True)\n",
    "    \n",
    "    # Align columns with training data\n",
    "    for col in X_train_dt2.columns:\n",
    "        if col not in client_cat_encoded.columns and col not in client_num.columns:\n",
    "            client_cat_encoded[col] = 0\n",
    "    \n",
    "    client_encoded = pd.concat([client_num, client_cat_encoded], axis=1)\n",
    "    client_encoded = client_encoded[X_train_dt2.columns]  # Ensure same column order\n",
    "else:\n",
    "    # DT1 - numeric only\n",
    "    client_df = pd.DataFrame([client_data])\n",
    "    client_encoded = client_df[numeric_features_only]\n",
    "\n",
    "# Predict using pruned model\n",
    "predicted_max_loan = pruned_model.predict(client_encoded)[0]\n",
    "\n",
    "print(f\"Client ID: 60256\")\n",
    "print(f\"Predicted Maximum Loan Amount: £{predicted_max_loan:,.2f}\")\n",
    "print(\"\\nNote: This prediction uses the pruned model (max_depth=4)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# COURSEWORK COMPLETE\n",
    "---\n",
    "\n",
    "## Summary\n",
    "\n",
    "### Part A: Classification (Loan Approval Prediction)\n",
    "- **Models Built:** Naive Bayes, Logistic Regression, Random Forest\n",
    "- **Best Model:** Selected based on Recall and Precision for \"Declined\" class\n",
    "- **Tuning:** GridSearchCV with 5-fold cross-validation\n",
    "- **Key Metrics:** Recall, Precision, F-Score, AUC-ROC\n",
    "\n",
    "### Part B: Regression (Maximum Loan Amount Prediction)\n",
    "- **Models Built:** DT1 (numeric features), DT2 (all features)\n",
    "- **Best Model:** Selected based on R² score\n",
    "- **Pruning:** Applied max_depth=4 for interpretability\n",
    "- **Prediction:** Client 60256 maximum loan amount estimated\n",
    "\n",
    "## Next Steps for Student Report\n",
    "1. Take screenshots of all outputs (statistical tables, plots, confusion matrices, metrics)\n",
    "2. Paste screenshots into report document\n",
    "3. Add brief interpretations for each task\n",
    "4. Complete the summary tables as shown in coursework instructions\n",
    "5. Submit report (max 23 pages) and this .ipynb file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TASK 4.b.i: Train-test split with reproducibility\n",
    "\n",
    "# Split for DT1 (numeric only)\n",
    "X_train_dt1, X_test_dt1, y_train_reg, y_test_reg = train_test_split(\n",
    "    X_dt1, y_reg, test_size=0.2, random_state=42  # Ensures reproducibility\n",
    ")\n",
    "\n",
    "# Split for DT2 (all features) - using same random_state for consistency\n",
    "X_train_dt2, X_test_dt2, y_train_reg2, y_test_reg2 = train_test_split(\n",
    "    X_dt2, y_reg, test_size=0.2, random_state=42  # Same random_state = same split\n",
    ")\n",
    "\n",
    "print(\"TASK 4.b.i: Reproducibility ensured with random_state=42\")\n",
    "print(\"\\nTASK 4.b.ii: Dataset dimensions:\\n\")\n",
    "\n",
    "print(\"DT1 (Numeric only):\")\n",
    "print(f\"  Training set: {X_train_dt1.shape}\")\n",
    "print(f\"  Test set: {X_test_dt1.shape}\")\n",
    "print(f\"  Features: {list(X_train_dt1.columns)}\\n\")\n",
    "\n",
    "print(\"DT2 (All features):\")\n",
    "print(f\"  Training set: {X_train_dt2.shape}\")\n",
    "print(f\"  Test set: {X_test_dt2.shape}\")\n",
    "print(f\"  Features: {list(X_train_dt2.columns)}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
